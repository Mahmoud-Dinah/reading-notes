## Web Scraping:

Web scraping is a technique to automatically access and extract large amounts of information from a website, which can save a huge amount of time and effort. In this article, we will go through an easy example of how to automate downloading hundreds of files from the New York MTA. This is a great exercise for web scraping beginners who are looking to understand how to web scrape. Web scraping can be slightly intimidating, so this tutorial will break down the process of how to go about the process.

## Important notes about web scraping:

Read through the website’s Terms and Conditions to understand how you can legally use the data. Most sites prohibit you from using the data for commercial purposes.
Make sure you are not downloading data at too rapid a rate because this may break the website. You may potentially be blocked from the site as well.

## Inspecting the Website:

The first thing that we need to do is to figure out where we can locate the links to the files we want to download inside the multiple levels of HTML tags.-

------------------------------

## Python Code

We start by importing the following libraries.
import requests
import urllib.request
import time
from bs4 import BeautifulSoup
Next, we set the url to the website and access the site with our requests library.

url = 'http://web.mta.info/developers/turnstile.html'.
response = requests.get(url)
If the access was successful, you should see the following output:

Next we parse the html with BeautifulSoup so that we can work with a nicer, nested BeautifulSoup data structure. If you are interested in learning more about this library, check out the BeatifulSoup documentation.

soup = BeautifulSoup(response.text, “html.parser”)
We use the method .findAll to locate all of our <a> tags.

soup.findAll('a')
This code gives us every line of code that has an <a> tag

-------------------------------
## Web scraping:

is data scraping used for extracting data from websites. The web scraping software may directly access the World Wide Web using the Hypertext Transferd Protocol or a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.

----------------------

## How to scrape websites without getting blocked:

## Respect Robots.txt

Web spiders should ideally follow the robot.txt file for a website while scraping. It has specific rules for good behavior such as how frequently you can scrape, which pages allow scraping, and which ones you can’t. Some websites allow Google to scrape their websites, by not allowing any other websites to scrape. This goes against the open nature of the Internet and may not seem fair but the owners of the website are within their rights to resort to such behavior. 

### **Do not follow the same crawling pattern**

### *Make requests through Proxies and rotate them as needed*

### *Rotate User Agents and corresponding HTTP Request Headers between requests*

### *Use a headless browser like Puppeteer, Selenium or Playwright*