## Data Science Primer

## What makes machine learning so special?:


Machine learning is the practice of teaching computers how to learn patterns from data, often for making decisions or predictions.

For true machine learning, the computer must be able to learn patterns that it's not explicitly programmed to identify.

Example: the curious child
A young child is playing at home... And he sees a candle! He cautiously waddles over.

Out of curiosity, he sticks his hand over the candle flame.
"Ouch!," he yells, as he yanks his hand back.
"Hmm... that red and bright thing really hurts!"
Ooh a candle!
Ooh a candle!

Two days later, he's playing in the kitchen... And he sees a stove-top! Again, he cautiously waddles over.

He's curious again, and he's thinking about sticking his hand over it.
Suddenly, he notices that it's red and bright!
"Ahh..." he thinks to himself, "not today!"
He remembers that red and bright means pain, and he ignores the stove top.
To be clear, it's only machine learning because the child learned patterns from the candle.

---------------------

## Machine Learning Tasks:

A task is a specific objective for your algorithms.

Algorithms can be swapped in and out, as long as you pick the right task.

In fact, you should always try multiple algorithms because you most likely won't know which one will perform best for your dataset.

----------------------

## Data Cleaning:


Data cleaning is one those things that everyone does but no one really talks about. Sure, it’s not the "sexiest" part of machine learning. And no, there aren’t hidden tricks and secrets to uncover.

However, proper data cleaning can make or break your project. Professional data scientists usually spend a very large portion of their time on this step.

--------------------
Machine learning requires computers to teach themselves how to make decisions based on patterns from data
topic|definition
----|-----
model | a set of patterns learned from data
algorithm | a specific ML process used to train a model
training data | the dataset from which the algorithm learns the model
test data | new dataset for evaluating model performance
features | variables (columns) in dataset used to train model
target variable | specific variable you're trying to predict
observation | data points (rows) in the dataset

---------------------

## Remove Unwanted observations:

The first step to data cleaning is removing unwanted observations from your dataset.

This includes duplicate or irrelevant observations.

### **Duplicate observations:**

Duplicate observations most frequently arise during data collection, such as when you:

Combine datasets from multiple places
Scrape data
Receive data from clients/other departments


### **Irrelevant observations:**

Irrelevant observations are those that don’t actually fit the specific problem that you’re trying to solve.

For example, if you were building a model for Single-Family homes only, you wouldn't want observations for Apartments in there.
This is also a great time to review your charts from Exploratory Analysis. You can look at the distribution charts for categorical features to see if there are any classes that shouldn’t be there.
Checking for irrelevant observations before engineering features can save you many headaches down the road.

-------------------------------------

## Feature Engineering

- create new input features from your existing ones

- think of it as process of addition compared to data cleaning which is process of subtraction

- think of information you want to isolate, it can be open-ended

    - create interaction features (two or more features together)
    - this can be products, sums, or differences between two features

- combine sparse classes

- remove unused features

-------------------------------------

## Algorithm Selection:

- linear regression can be flaws

    - it is prone to overfit with many input features.
    - it cannot easily exwpress non-linear relationships.

- use regularization to prevent overfitting

- types of regularized regression algos:
    - lasso regression
    - ridge regression
    - elastic-net

- tree ensembling:

    - bagging: attempts to reduce chance of overfitting complex models
    - boosting: attempts to improve predictive flexibility of simple models